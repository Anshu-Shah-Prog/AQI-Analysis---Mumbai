import kagglehub

# Download latest version
path = kagglehub.dataset_download("gyaswanth297/air-quality-indexaqi-data-of-mumbai")

print("Path to dataset files:", path)


df1 = pd.read_csv(path+  "/BandraKurlaComplexMumbaiIITM.csv")
df2 = pd.read_csv(path+  "/BandraMumbaiMPCB.csv")
df3 = pd.read_csv(path+  "/BorivaliEastMumbaiIITM.csv")
df4 = pd.read_csv(path+  "/BorivaliEastMumbaiMPCB.csv")
df5 = pd.read_csv(path+  "/ChakalaAndheriEastMumbaiIITM.csv")
df6 = pd.read_csv(path+  "/ChhatrapatiShivajiIntlAirportT2MumbaiMPCB.csv")
df7 = pd.read_csv(path+  "/ColabaMumbaiMPCB.csv")
df8 = pd.read_csv(path+  "/DeonarMumbaiIITM.csv")
df9 = pd.read_csv(path+  "/KandivaliEastMumbaiMPCB.csv")
df10 = pd.read_csv(path+  "/KhindipadaBhandupWestMumbaiIITM.csv")
df11 = pd.read_csv(path+  "/KurlaMumbaiMPCB.csv")
df12 = pd.read_csv(path+  "/MaladWestMumbaiIITM.csv")
df13 = pd.read_csv(path+  "/MazgaonMumbaiIITM.csv")
df14 = pd.read_csv(path+  "/MulundWestMumbaiMPCB.csv")
df15 = pd.read_csv(path+  "/NavyNagarColabaMumbaiIITM.csv")
df16 = pd.read_csv(path+  "/PowaiMumbaiMPCB.csv")
df17 = pd.read_csv(path+  "/SiddharthNagarWorliMumbaiIITM.csv")
df18 = pd.read_csv(path+  "/SionMumbaiMPCB.csv")
df19 = pd.read_csv(path+  "/VasaiWestMumbaiMPCB.csv")
df20 = pd.read_csv(path+  "/VileParleWestMumbaiMPCB.csv")
df21 = pd.read_csv(path+  "/WorliMumbaiMPCB.csv")

# list of all dataframes
dfs = [
    df1, df2, df3, df4, df5, df6, df7, df8, df9, df10,
    df11, df12, df13, df14, df15, df16, df17, df18, df19, df20, df21
]

# list of dataset names for reference
df_names = [
    "BandraKurlaComplexMumbaiIITM",
    "BandraMumbaiMPCB",
    "BorivaliEastMumbaiIITM",
    "BorivaliEastMumbaiMPCB",
    "ChakalaAndheriEastMumbaiIITM",
    "ChhatrapatiShivajiIntlAirportT2MumbaiMPCB",
    "ColabaMumbaiMPCB",
    "DeonarMumbaiIITM",
    "KandivaliEastMumbaiMPCB",
    "KhindipadaBhandupWestMumbaiIITM",
    "KurlaMumbaiMPCB",
    "MaladWestMumbaiIITM",
    "MazgaonMumbaiIITM",
    "MulundWestMumbaiMPCB",
    "NavyNagarColabaMumbaiIITM",
    "PowaiMumbaiMPCB",
    "SiddharthNagarWorliMumbaiIITM",
    "SionMumbaiMPCB",
    "VasaiWestMumbaiMPCB",
    "VileParleWestMumbaiMPCB",
    "WorliMumbaiMPCB"
]

# loop and print column names
for name, df in zip(df_names, dfs):
    print(f"\nðŸ“Š Columns in {name}:")
    print(list(df.shape))

# pollutants of interest
aqi_columns = ["Source","From Date", "To Date", "PM2.5", "PM10", "NO2", "SO2", "CO", "Ozone", "NH3"]

# list to collect filtered dataframes
combined_list = []

for name, df in zip(df_names, dfs):
    # keep only the pollutant columns that exist in the dataset
    available_cols = [col for col in aqi_columns if col in df.columns]
    temp_df = df[available_cols].copy()

    # add the source column
    temp_df["Source"] = name

    # append to list
    combined_list.append(temp_df)

# merge all into one dataframe
combined_df = pd.concat(combined_list, ignore_index=True)

print(combined_df.head())
print("\nShape of combined DataFrame:", combined_df.shape)

combined_df.isnull().sum()

# Convert datetime columns
combined_df["From Date"] = pd.to_datetime(combined_df["From Date"], errors="coerce")
combined_df["To Date"]   = pd.to_datetime(combined_df["To Date"], errors="coerce")

# Use 'From Date' as the timestamp
combined_df["Datetime"] = combined_df["From Date"]

# Extract date only
combined_df["Date"] = combined_df["Datetime"].dt.date

pollutant_cols = ["PM2.5", "PM10", "NO2", "SO2", "CO", "Ozone", "NH3"]

for col in pollutant_cols:
    if col in combined_df.columns:
        combined_df[col] = pd.to_numeric(combined_df[col], errors="coerce")

 # Daily averages per station
daily_df = (
    combined_df.groupby(["Source", "Date"])[pollutant_cols]
      .mean()
      .reset_index()
)

print(daily_df.head())
print("\nShape of daily dataset:", daily_df.shape)

daily_df.head()
daily_df.isnull().sum()
daily_df.ffill(axis='rows', inplace=True)
daily_df.isnull().sum()
daily_df = (
    combined_df.groupby(["Date"])[pollutant_cols]
      .mean()
      .reset_index()
)
print("\nShape of daily dataset:", daily_df.shape)

daily_df["Date"] = pd.to_datetime(daily_df["Date"])
final_df = daily_df[daily_df["Date"] <= pd.to_datetime("2023-01-31")]
final_df.shape

# Example: final_df has columns ['Date', 'PM2.5','PM10','NO2','SO2','CO','O3','NH3','Source']

# Define breakpoints (simplified, use exact CPCB values in practice)
breakpoints = {
    "PM2.5": [(0,30,0,50),(31,60,51,100),(61,90,101,200),(91,120,201,300),(121,250,301,400),(251,350,401,500)],
    "PM10": [(0,50,0,50),(51,100,51,100),(101,250,101,200),(251,350,201,300),(351,430,301,400),(431,600,401,500)],
    "NO2": [(0,40,0,50),(41,80,51,100),(81,180,101,200),(181,280,201,300),(281,400,301,400),(401,1000,401,500)],
    "SO2": [(0,40,0,50),(41,80,51,100),(81,380,101,200),(381,800,201,300),(801,1600,301,400),(1601,2620,401,500)],
    "CO": [(0,1,0,50),(1.1,2,51,100),(2.1,10,101,200),(10.1,17,201,300),(17.1,34,301,400),(34.1,50,401,500)],
    "Ozone": [(0,50,0,50),(51,100,51,100),(101,168,101,200),(169,208,201,300),(209,748,301,400),(749,1000,401,500)],
    "NH3": [(0,200,0,50),(201,400,51,100),(401,800,101,200),(801,1200,201,300),(1201,1800,301,400),(1801,2500,401,500)]
}

# AQI categories
aqi_categories = [
    (0,50,"Good"),
    (51,100,"Satisfactory"),
    (101,200,"Moderate"),
    (201,300,"Poor"),
    (301,400,"Very Poor"),
    (401,500,"Severe")
]

# Function to calculate sub-index for one pollutant
def calculate_subindex(pollutant, conc):
    for (C_low, C_high, I_low, I_high) in breakpoints[pollutant]:
        if C_low <= conc <= C_high:
            # linear interpolation
            return (I_high - I_low)/(C_high - C_low) * (conc - C_low) + I_low
    return np.nan

# Copy dataframe to avoid overwriting
aqi_df = final_df.copy()

# Calculate sub-indices for all pollutants
for pollutant in breakpoints.keys():
    if pollutant in aqi_df.columns:
        aqi_df[pollutant+"_SI"] = aqi_df[pollutant].apply(lambda x: calculate_subindex(pollutant, x))

# Final AQI = maximum of all sub-indices per row
si_cols = [p+"_SI" for p in breakpoints.keys()]
aqi_df["AQI"] = aqi_df[si_cols].max(axis=1)

# Assign AQI category
def aqi_category(aqi):
    for low, high, cat in aqi_categories:
        if low <= aqi <= high:
            return cat
    return "Unknown"

aqi_df["AQI_Category"] = aqi_df["AQI"].apply(aqi_category)

print(aqi_df[["Date","AQI","AQI_Category"]].head())

aqi_df.head()

 Filter for one station
# station_df = aqi_df[aqi_df["Source"] == "BandraKurlaComplexMumbaiIITM"]

# Set datetime index
station_df = aqi_df.copy()
station_df["Date"] = pd.to_datetime(station_df["Date"])
ts_df = station_df.set_index("Date")  # index = Date


# Choose the series for analysis
ts = ts_df["AQI"]  # or AQI if you have calculated
station_df[['Date', 'AQI']].to_csv('aqi_bandra.csv', index=False)

ts.plot(figsize=(12,5), title="AQI Time Series")
plt.xlabel("Date")
plt.ylabel("AQI")
plt.show()

from statsmodels.tsa.stattools import adfuller, kpss

adf_result = adfuller(ts)
print("ADF Statistic:", adf_result[0])
print("p-value:", adf_result[1])
print("Critical Values:", adf_result[4])

if adf_result[1] < 0.05:
    print("ADF Test: Series is likely stationary")
else:
    print("ADF Test: Series is likely non-stationary")

kpss_result = kpss(ts, regression='c', nlags="auto")  # 'c' for constant
print("\nKPSS Statistic:", kpss_result[0])
print("p-value:", kpss_result[1])
print("Critical Values:", kpss_result[3])

if kpss_result[1] < 0.05:
    print("KPSS Test: Series is likely non-stationary")
else:
    print("KPSS Test: Series is likely stationary")

from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# Create subplots
fig, axes = plt.subplots(1, 2, figsize=(14,5))

# ACF plot
plot_acf(ts, lags=30, ax=axes[0])
axes[0].set_title("ACF - AQI")

# PACF plot
plot_pacf(ts, lags=30, ax=axes[1], method='ywm')
axes[1].set_title("PACF - AQI")

plt.tight_layout()
plt.show()

from statsmodels.tsa.arima.model import ARIMA
# Define ARIMA model with order (p,d,q) = (1,1,1)
model = ARIMA(ts, order=(1,1,1))

# Fit the model
model_fit = model.fit()

# Print summary
print(model_fit.summary())

# In-sample prediction
ts_pred = model_fit.predict(start=ts.index[0], end=ts.index[-1])

# Plot actual vs predicted
plt.figure(figsize=(12,5))
plt.plot(ts, label="Actual AQI")
plt.plot(ts_pred, label="Predicted AQI", color="red")
plt.title("ARIMA(1,0,0) Fit")
plt.xlabel("Date")
plt.ylabel("PM2.5")
plt.legend()
plt.show()

import pandas as pd
import itertools
import warnings

# Define ranges for p, d, q
p = range(0, 3)   # AR terms
d = range(0, 2)   # differencing
q = range(0, 3)   # MA terms

warnings.filterwarnings("ignore")
# Generate all combinations
pdq = list(itertools.product(p, d, q))

best_aic = float("inf")
best_order = None
best_model = None

for order in pdq:
    try:
        model = ARIMA(ts, order=order)
        model_fit = model.fit()
        if model_fit.aic < best_aic:
            best_aic = model_fit.aic
            best_order = order
            best_model = model_fit
    except:
        continue

print(f"Best ARIMA order: {best_order}, AIC: {best_aic}")
print(best_model.summary())

import matplotlib.pyplot as plt

# Get residuals
residuals = best_model.resid

# Plot residuals over time
plt.figure(figsize=(12,6))
plt.subplot(2,1,1)
plt.plot(residuals)
plt.title("Residuals over Time")
plt.xlabel("Date")
plt.ylabel("Residuals")

# Plot histogram of residuals
plt.subplot(2,1,2)
plt.hist(residuals, bins=30, edgecolor='k')
plt.title("Residuals Histogram")
plt.xlabel("Residual value")
plt.ylabel("Frequency")

plt.tight_layout()
plt.show()


import pandas as pd
import matplotlib.pyplot as plt

# Number of days to forecast
n_forecast = 90

# Forecast
forecast = best_model.get_forecast(steps=n_forecast)
forecast_values = forecast.predicted_mean
conf_int = forecast.conf_int()  # 95% confidence intervals

# Create future dates
last_date = ts.index[-1]
future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=n_forecast)
forecast_values.index = future_dates
conf_int.index = future_dates

# Plot historical + forecast
plt.figure(figsize=(12,6))
plt.plot(ts, label="Historical AQI")
plt.plot(forecast_values, label="Forecast AQI", color='red')
plt.fill_between(conf_int.index,
                 conf_int.iloc[:,0], conf_int.iloc[:,1],
                 color='pink', alpha=0.3, label="95% Confidence Interval")
plt.title(f"ARIMA{best_order} Forecast for Next 30 Days")
plt.xlabel("Date")
plt.ylabel("AQI")
plt.legend()
plt.show()

daily_df = pd.read_csv("/content/aqi_bandra (2).csv")
daily_df.head()
# Convert Date column to datetime
daily_df["Date"] = pd.to_datetime(daily_df["Date"])
# Set Date as index
daily_df = daily_df.set_index("Date")

ts = daily_df["AQI"].asfreq("D")

ts = ts[ts.index < "2022-12-31"]

from statsmodels.tsa.statespace.sarimax import SARIMAX

sarima_model = SARIMAX(ts, order=(1,1,1), seasonal_order=(1,1,1,7))
sarima_fit = sarima_model.fit(disp=False)

sarima_forecast = sarima_fit.get_forecast(steps=30)
sarima_pred = sarima_forecast.predicted_mean
print(sarima_fit.summary())

# Forecast next 30 days
import matplotlib.pyplot as plt

n_forecast = 30
forecast = sarima_fit.get_forecast(steps=n_forecast)
forecast_values = forecast.predicted_mean
conf_int = forecast.conf_int()

# Create future dates
future_dates = pd.date_range(start=ts.index[-1] + pd.Timedelta(days=1), periods=n_forecast)
forecast_values.index = future_dates
conf_int.index = future_dates

# Plot
plt.figure(figsize=(12,6))
plt.plot(ts, label="Historical AQI")
plt.plot(forecast_values, label="SARIMA Forecast", color="red")
plt.fill_between(conf_int.index, conf_int.iloc[:,0], conf_int.iloc[:,1],
                 color="pink", alpha=0.3, label="95% Confidence Interval")
plt.title("SARIMA Forecast of Daily AQI (Next 30 Days)")
plt.xlabel("Date")
plt.ylabel("AQI")
plt.legend()
plt.show()

from prophet import Prophet

# Prepare dataframe
prophet_df = ts.reset_index()
prophet_df.columns = ["ds", "y"]  # Prophet needs 'ds' and 'y'

# Fit model
model = Prophet(daily_seasonality=True, yearly_seasonality=True)
model.fit(prophet_df)

# Forecast next 30 days
future = model.make_future_dataframe(periods=90)
forecast = model.predict(future)

# Plot
model.plot(forecast)
model.plot_components(forecast)

from prophet.plot import plot_plotly, plot_components_plotly

plot_plotly(model, forecast)
plot_components_plotly(model, forecast)

import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from sklearn.preprocessing import MinMaxScaler

# Scale data
scaler = MinMaxScaler(feature_range=(0,1))
scaled_ts = scaler.fit_transform(ts.values.reshape(-1,1))

# Prepare sequences
X, y = [], []
time_steps = 30  # use past 30 days to predict next day
for i in range(len(scaled_ts)-time_steps):
    X.append(scaled_ts[i:i+time_steps])
    y.append(scaled_ts[i+time_steps])
X, y = np.array(X), np.array(y)

# Build model
model = Sequential()
model.add(LSTM(64, return_sequences=True, input_shape=(X.shape[1], X.shape[2])))
model.add(Dropout(0.2))
model.add(LSTM(64))
model.add(Dense(1))
model.compile(optimizer="adam", loss="mse")

# Train
model.fit(X, y, epochs=20, batch_size=16, verbose=1)

# Forecast next 30 days
last_seq = scaled_ts[-time_steps:]
preds = []
for _ in range(30):
    pred = model.predict(last_seq.reshape(1, time_steps, 1))
    preds.append(pred[0,0])
    last_seq = np.vstack([last_seq[1:], pred])

preds = scaler.inverse_transform(np.array(preds).reshape(-1,1))


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error, mean_squared_error
from statsmodels.tsa.statespace.sarimax import SARIMAX
from statsmodels.tsa.seasonal import seasonal_decompose
from prophet import Prophet
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from sklearn.preprocessing import MinMaxScaler
import warnings
warnings.filterwarnings("ignore")

# -------------------------------
# 1. Load dataset
# # -------------------------------
# df = pd.read_csv("/content/aqi_bandra (2).csv")  # replace with your file
# df["Date"] = pd.to_datetime(df["Date"])
# df = df.set_index("Date")
# ts = df["AQI"].asfreq("D")  # ensure daily frequency

# -------------------------------
# 2. Seasonal Decomposition
# -------------------------------
decomp = seasonal_decompose(ts, model="additive", period=364)  # weekly seasonality
fig = decomp.plot()
fig.set_size_inches(10, 10)
plt.suptitle("Seasonal Decomposition of Daily AQI", fontsize=16)
plt.show()

# -------------------------------
# Helper: Evaluation function
# -------------------------------
def evaluate(true, pred):
    mae = mean_absolute_error(true, pred)
    rmse = np.sqrt(mean_squared_error(true, pred))
    mape = np.mean(np.abs((true - pred) / true)) * 100
    return mae, rmse, mape

# -------------------------------
# 3. SARIMA
# -------------------------------
sarima_model = SARIMAX(ts, order=(1,1,1), seasonal_order=(1,1,1,7))
sarima_fit = sarima_model.fit(disp=False)

sarima_forecast = sarima_fit.get_forecast(steps=90)
sarima_pred = sarima_forecast.predicted_mean

# -------------------------------
# 4. Prophet
# -------------------------------
prophet_df = ts.reset_index()
prophet_df.columns = ["ds", "y"]

prophet_model = Prophet(daily_seasonality=True, yearly_seasonality=True)
prophet_model.fit(prophet_df)

future = prophet_model.make_future_dataframe(periods=90)
prophet_forecast = prophet_model.predict(future)
prophet_pred = prophet_forecast.tail(90)["yhat"].values
prophet_pred = pd.Series(prophet_pred, index=pd.date_range(ts.index[-1] + pd.Timedelta(days=1), periods=90))

# -------------------------------
# 5. LSTM
# -------------------------------
# Scale data
scaler = MinMaxScaler(feature_range=(0,1))
scaled_ts = scaler.fit_transform(ts.values.reshape(-1,1))

# Create sequences
time_steps = 90
X, y = [], []
for i in range(len(scaled_ts)-time_steps):
    X.append(scaled_ts[i:i+time_steps])
    y.append(scaled_ts[i+time_steps])
X, y = np.array(X), np.array(y)

# Build model
model = Sequential()
model.add(LSTM(64, return_sequences=True, input_shape=(X.shape[1], X.shape[2])))
model.add(Dropout(0.2))
model.add(LSTM(64))
model.add(Dense(1))
model.compile(optimizer="adam", loss="mse")

# Train
model.fit(X, y, epochs=20, batch_size=16, verbose=0)

# Forecast
last_seq = scaled_ts[-time_steps:]
lstm_preds = []
for _ in range(90):
    pred = model.predict(last_seq.reshape(1, time_steps, 1), verbose=0)
    lstm_preds.append(pred[0,0])
    last_seq = np.vstack([last_seq[1:], pred])

lstm_pred = scaler.inverse_transform(np.array(lstm_preds).reshape(-1,1)).flatten()
lstm_pred = pd.Series(lstm_pred, index=pd.date_range(ts.index[-1] + pd.Timedelta(days=1), periods=90))

# -------------------------------
# 6. Evaluation (using last 30 days as test)
# -------------------------------
train = ts[:-90]
test = ts[-90:]

# SARIMA test forecast
sarima_test_forecast = sarima_fit.get_forecast(steps=90).predicted_mean
mae_s, rmse_s, mape_s = evaluate(test, sarima_test_forecast)

# Prophet test forecast
prophet_test = prophet_model.predict(prophet_df.tail(90))[["yhat"]]
mae_p, rmse_p, mape_p = evaluate(test.values, prophet_test["yhat"].values)

# LSTM evaluation
mae_l, rmse_l, mape_l = evaluate(test.values, lstm_pred.values[:90])

results = pd.DataFrame({
    "Model": ["SARIMA", "Prophet", "LSTM"],
    "MAE": [mae_s, mae_p, mae_l],
    "RMSE": [rmse_s, rmse_p, rmse_l],
    "MAPE": [mape_s, mape_p, mape_l]
})

print("Model Comparison:\n", results)

# Combine the last historical point with each forecast for a continuous plot
sarima_combined_plot = pd.concat([ts.tail(2), sarima_pred])
prophet_combined_plot = pd.concat([ts.tail(2), prophet_pred])
lstm_combined_plot = pd.concat([ts.tail(2), lstm_pred])

# -------------------------------
# 7. Plot Forecasts
# -------------------------------
plt.figure(figsize=(12,6))
plt.plot(ts, label="Historical AQI", color="green")
plt.plot(sarima_combined_plot, label="SARIMA Forecast", color="red")
plt.plot(prophet_combined_plot, label="Prophet Forecast", color="yellow")
plt.plot(lstm_combined_plot, label="LSTM Forecast", color="blue")
plt.title("AQI Forecasting (Next 90 Days)")
plt.xlabel("Date")
plt.ylabel("AQI")
plt.legend()
plt.show()


# =========================
# Seasonal Decomposition (7-day)
# =========================
decomposition = seasonal_decompose(ts, model="additive", period=7)

fig = decomposition.plot()
fig.set_size_inches(12, 8)
plt.suptitle("Seasonal Decomposition of AQI (7-day seasonality)", fontsize=14)
plt.show()

# =========================
# ACF & PACF plots
# =========================
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

plot_acf(ts.dropna(), lags=40, ax=axes[0], color="green")
axes[0].set_title("Autocorrelation (ACF)")

plot_pacf(ts.dropna(), lags=40, ax=axes[1], color="green", method='ywm')
axes[1].set_title("Partial Autocorrelation (PACF)")

plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# Set a stylish color for the plots
plot_color = 'forestgreen'
line_color = '#228B22'

# Seasonal Decomposition Plot
print("Generating Seasonal Decomposition Plot...")
result = seasonal_decompose(ts, model='additive', period=7)

plt.figure(figsize=(12, 10))
plt.style.use('seaborn-v0_8-whitegrid')

# Observed
ax1 = plt.subplot(4, 1, 1)
ax1.set_title('Observed', color=line_color, fontsize=14, fontweight='bold')
ax1.plot(result.observed, color=line_color, linewidth=2)
ax1.set_facecolor('#f5f5f5')
plt.grid(True, which='both', linestyle='--', linewidth=0.5, color='gray')

# Trend
ax2 = plt.subplot(4, 1, 2)
ax2.set_title('Trend', color=line_color, fontsize=14, fontweight='bold')
ax2.plot(result.trend, color=line_color, linewidth=2)
ax2.set_facecolor('#f5f5f5')
plt.grid(True, which='both', linestyle='--', linewidth=0.5, color='gray')

# Seasonal
ax3 = plt.subplot(4, 1, 3)
ax3.set_title('Seasonal', color=line_color, fontsize=14, fontweight='bold')
ax3.plot(result.seasonal, color=line_color, linewidth=2)
ax3.set_facecolor('#f5f5f5')
plt.grid(True, which='both', linestyle='--', linewidth=0.5, color='gray')

# Residual
ax4 = plt.subplot(4, 1, 4)
ax4.set_title('Residual', color=line_color, fontsize=14, fontweight='bold')
ax4.plot(result.resid, color=line_color, marker='.', markersize=2, linestyle='None')
ax4.set_facecolor('#f5f5f5')
plt.grid(True, which='both', linestyle='--', linewidth=0.5, color='gray')

plt.tight_layout(pad=2.0)
plt.show()

# ACF and PACF Plots
print("\nGenerating ACF and PACF Plots...")
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))
plt.style.use('seaborn-v0_8-whitegrid')

# ACF Plot
plot_acf(ts.dropna(), ax=ax1, lags=50, color=plot_color, title='Autocorrelation Function (ACF)',
         alpha=0.05, marker='o', markersize=6)
ax1.set_facecolor('#f5f5f5')
ax1.axhline(y=0, color='gray', linestyle='--')
ax1.set_title('Autocorrelation Function (ACF)', fontsize=14, fontweight='bold', color=line_color)
ax1.grid(True, which='both', linestyle='--', linewidth=0.5, color='gray')

# PACF Plot
plot_pacf(ts.dropna(), ax=ax2, lags=50, color=plot_color, title='Partial Autocorrelation Function (PACF)',
          alpha=0.05, marker='o', markersize=6)
ax2.set_facecolor('#f5f5f5')
ax2.axhline(y=0, color='gray', linestyle='--')
ax2.set_title('Partial Autocorrelation Function (PACF)', fontsize=14, fontweight='bold', color=line_color)
ax2.grid(True, which='both', linestyle='--', linewidth=0.5, color='gray')

plt.tight_layout(pad=2.0)
plt.show()
